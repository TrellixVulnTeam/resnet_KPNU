{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "DuplicateFlagError",
     "evalue": "The flag 'data_dir' is defined twice. First from official.utils.flags._base, Second from official.utils.flags._base.  Description from first occurrence: \nThe location of the input data.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mDuplicateFlagError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-437bbd74cecc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m   \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_verbosity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mINFO\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m   \u001b[0mdefine_cifar_flags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    262\u001b[0m   \u001b[0mabsl_app\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-437bbd74cecc>\u001b[0m in \u001b[0;36mdefine_cifar_flags\u001b[0;34m()\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdefine_cifar_flags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m   \u001b[0mresnet_run_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefine_resnet_flags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m   \u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madopt_module_key_flags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresnet_run_loop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m   flags_core.set_defaults(data_dir='/tmp/cifar10_data',\n",
      "\u001b[0;32m~/models/official/resnet/resnet_run_loop.py\u001b[0m in \u001b[0;36mdefine_resnet_flags\u001b[0;34m(resnet_size_choices)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdefine_resnet_flags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresnet_size_choices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m   \u001b[0;34m\"\"\"Add flags and validators for ResNet.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m   \u001b[0mflags_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefine_base\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m   \u001b[0mflags_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefine_performance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_parallel_calls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m   \u001b[0mflags_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefine_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/models/official/utils/flags/core.py\u001b[0m in \u001b[0;36mcore_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcore_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m     \u001b[0mkey_flags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;34m[\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeclare_key_flag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfl\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkey_flags\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# pylint: disable=expression-not-assigned\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mcore_fn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/models/official/utils/flags/_base.py\u001b[0m in \u001b[0;36mdefine_base\u001b[0;34m(data_dir, model_dir, train_epochs, epochs_between_evals, stop_threshold, batch_size, multi_gpu, num_gpu, hooks, export_dir)\u001b[0m\n\u001b[1;32m     52\u001b[0m     flags.DEFINE_string(\n\u001b[1;32m     53\u001b[0m         \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"data_dir\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshort_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"dd\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"/tmp\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         help=help_wrap(\"The location of the input data.\"))\n\u001b[0m\u001b[1;32m     55\u001b[0m     \u001b[0mkey_flags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data_dir\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/absl/flags/_defines.py\u001b[0m in \u001b[0;36mDEFINE_string\u001b[0;34m(name, default, help, flag_values, **args)\u001b[0m\n\u001b[1;32m    239\u001b[0m   \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_argument_parser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mArgumentParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m   \u001b[0mserializer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_argument_parser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mArgumentSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m   \u001b[0mDEFINE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhelp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mserializer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/absl/flags/_defines.py\u001b[0m in \u001b[0;36mDEFINE\u001b[0;34m(parser, name, default, help, flag_values, serializer, module_name, **args)\u001b[0m\n\u001b[1;32m     80\u001b[0m   \"\"\"\n\u001b[1;32m     81\u001b[0m   DEFINE_flag(_flag.Flag(parser, serializer, name, default, help, **args),\n\u001b[0;32m---> 82\u001b[0;31m               flag_values, module_name)\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/absl/flags/_defines.py\u001b[0m in \u001b[0;36mDEFINE_flag\u001b[0;34m(flag, flag_values, module_name)\u001b[0m\n\u001b[1;32m    102\u001b[0m   \u001b[0;31m# Copying the reference to flag_values prevents pychecker warnings.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m   \u001b[0mfv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflag_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m   \u001b[0mfv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mflag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m   \u001b[0;31m# Tell flag_values who's defining the flag.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/absl/flags/_flagvalues.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, name, flag)\u001b[0m\n\u001b[1;32m    425\u001b[0m         \u001b[0;31m# module is simply being imported a subsequent time.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0m_exceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDuplicateFlagError\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_flag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m     \u001b[0mshort_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshort_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m     \u001b[0;31m# If a new flag overrides an old one, we need to cleanup the old flag's\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDuplicateFlagError\u001b[0m: The flag 'data_dir' is defined twice. First from official.utils.flags._base, Second from official.utils.flags._base.  Description from first occurrence: \nThe location of the input data."
     ]
    }
   ],
   "source": [
    "# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "\"\"\"Runs a ResNet model on the CIFAR-10 dataset.\"\"\"\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "from absl import app as absl_app\n",
    "from absl import flags\n",
    "import tensorflow as tf  # pylint: disable=g-bad-import-order\n",
    "\n",
    "from official.utils.flags import core as flags_core\n",
    "from official.utils.logs import logger\n",
    "from official.resnet import resnet_model\n",
    "from official.resnet import resnet_run_loop\n",
    "\n",
    "\n",
    "_HEIGHT = 32\n",
    "_WIDTH = 32\n",
    "_NUM_CHANNELS = 3\n",
    "_DEFAULT_IMAGE_BYTES = _HEIGHT * _WIDTH * _NUM_CHANNELS\n",
    "# The record is the image plus a one-byte label\n",
    "_RECORD_BYTES = _DEFAULT_IMAGE_BYTES + 1\n",
    "_NUM_CLASSES = 10\n",
    "_NUM_DATA_FILES = 5\n",
    "\n",
    "_NUM_IMAGES = {\n",
    "    'train': 50000,\n",
    "    'validation': 10000,\n",
    "}\n",
    "\n",
    "DATASET_NAME = 'CIFAR-10'\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# Data processing\n",
    "###############################################################################\n",
    "def get_filenames(is_training, data_dir):\n",
    "  \"\"\"Returns a list of filenames.\"\"\"\n",
    "  data_dir = os.path.join(data_dir, 'cifar-10-batches-bin')\n",
    "\n",
    "  assert os.path.exists(data_dir), (\n",
    "      'Run cifar10_download_and_extract.py first to download and extract the '\n",
    "      'CIFAR-10 data.')\n",
    "\n",
    "  if is_training:\n",
    "    return [\n",
    "        os.path.join(data_dir, 'data_batch_%d.bin' % i)\n",
    "        for i in range(1, _NUM_DATA_FILES + 1)\n",
    "    ]\n",
    "  else:\n",
    "    return [os.path.join(data_dir, 'test_batch.bin')]\n",
    "\n",
    "\n",
    "def parse_record(raw_record, is_training):\n",
    "  \"\"\"Parse CIFAR-10 image and label from a raw record.\"\"\"\n",
    "  # Convert bytes to a vector of uint8 that is record_bytes long.\n",
    "  record_vector = tf.decode_raw(raw_record, tf.uint8)\n",
    "\n",
    "  # The first byte represents the label, which we convert from uint8 to int32\n",
    "  # and then to one-hot.\n",
    "  label = tf.cast(record_vector[0], tf.int32)\n",
    "\n",
    "  # The remaining bytes after the label represent the image, which we reshape\n",
    "  # from [depth * height * width] to [depth, height, width].\n",
    "  depth_major = tf.reshape(record_vector[1:_RECORD_BYTES],\n",
    "                           [_NUM_CHANNELS, _HEIGHT, _WIDTH])\n",
    "\n",
    "  # Convert from [depth, height, width] to [height, width, depth], and cast as\n",
    "  # float32.\n",
    "  image = tf.cast(tf.transpose(depth_major, [1, 2, 0]), tf.float32)\n",
    "\n",
    "  image = preprocess_image(image, is_training)\n",
    "\n",
    "  return image, label\n",
    "\n",
    "\n",
    "def preprocess_image(image, is_training):\n",
    "  \"\"\"Preprocess a single image of layout [height, width, depth].\"\"\"\n",
    "  if is_training:\n",
    "    # Resize the image to add four extra pixels on each side.\n",
    "    image = tf.image.resize_image_with_crop_or_pad(\n",
    "        image, _HEIGHT + 8, _WIDTH + 8)\n",
    "\n",
    "    # Randomly crop a [_HEIGHT, _WIDTH] section of the image.\n",
    "    image = tf.random_crop(image, [_HEIGHT, _WIDTH, _NUM_CHANNELS])\n",
    "\n",
    "    # Randomly flip the image horizontally.\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "\n",
    "  # Subtract off the mean and divide by the variance of the pixels.\n",
    "  image = tf.image.per_image_standardization(image)\n",
    "  return image\n",
    "\n",
    "\n",
    "def input_fn(is_training, data_dir, batch_size, num_epochs=1, num_gpus=None):\n",
    "  \"\"\"Input_fn using the tf.data input pipeline for CIFAR-10 dataset.\n",
    "\n",
    "  Args:\n",
    "    is_training: A boolean denoting whether the input is for training.\n",
    "    data_dir: The directory containing the input data.\n",
    "    batch_size: The number of samples per batch.\n",
    "    num_epochs: The number of epochs to repeat the dataset.\n",
    "    num_gpus: The number of gpus used for training.\n",
    "\n",
    "  Returns:\n",
    "    A dataset that can be used for iteration.\n",
    "  \"\"\"\n",
    "  filenames = get_filenames(is_training, data_dir)\n",
    "  dataset = tf.data.FixedLengthRecordDataset(filenames, _RECORD_BYTES)\n",
    "\n",
    "  return resnet_run_loop.process_record_dataset(\n",
    "      dataset=dataset,\n",
    "      is_training=is_training,\n",
    "      batch_size=batch_size,\n",
    "      shuffle_buffer=_NUM_IMAGES['train'],\n",
    "      parse_record_fn=parse_record,\n",
    "      num_epochs=num_epochs,\n",
    "      num_gpus=num_gpus,\n",
    "      examples_per_epoch=_NUM_IMAGES['train'] if is_training else None\n",
    "  )\n",
    "\n",
    "\n",
    "def get_synth_input_fn():\n",
    "  return resnet_run_loop.get_synth_input_fn(\n",
    "      _HEIGHT, _WIDTH, _NUM_CHANNELS, _NUM_CLASSES)\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# Running the model\n",
    "###############################################################################\n",
    "class Cifar10Model(resnet_model.Model):\n",
    "  \"\"\"Model class with appropriate defaults for CIFAR-10 data.\"\"\"\n",
    "\n",
    "  def __init__(self, resnet_size, data_format=None, num_classes=_NUM_CLASSES,\n",
    "               resnet_version=resnet_model.DEFAULT_VERSION,\n",
    "               dtype=resnet_model.DEFAULT_DTYPE):\n",
    "    \"\"\"These are the parameters that work for CIFAR-10 data.\n",
    "\n",
    "    Args:\n",
    "      resnet_size: The number of convolutional layers needed in the model.\n",
    "      data_format: Either 'channels_first' or 'channels_last', specifying which\n",
    "        data format to use when setting up the model.\n",
    "      num_classes: The number of output classes needed from the model. This\n",
    "        enables users to extend the same model to their own datasets.\n",
    "      resnet_version: Integer representing which version of the ResNet network\n",
    "      to use. See README for details. Valid values: [1, 2]\n",
    "      dtype: The TensorFlow dtype to use for calculations.\n",
    "\n",
    "    Raises:\n",
    "      ValueError: if invalid resnet_size is chosen\n",
    "    \"\"\"\n",
    "    if resnet_size % 6 != 2:\n",
    "      raise ValueError('resnet_size must be 6n + 2:', resnet_size)\n",
    "\n",
    "    num_blocks = (resnet_size - 2) // 6\n",
    "\n",
    "    super(Cifar10Model, self).__init__(\n",
    "        resnet_size=resnet_size,\n",
    "        bottleneck=False,\n",
    "        num_classes=num_classes,\n",
    "        num_filters=16,\n",
    "        kernel_size=3,\n",
    "        conv_stride=1,\n",
    "        first_pool_size=None,\n",
    "        first_pool_stride=None,\n",
    "        block_sizes=[num_blocks] * 3,\n",
    "        block_strides=[1, 2, 2],\n",
    "        final_size=64,\n",
    "        resnet_version=resnet_version,\n",
    "        data_format=data_format,\n",
    "        dtype=dtype\n",
    "    )\n",
    "\n",
    "\n",
    "def cifar10_model_fn(features, labels, mode, params):\n",
    "  \"\"\"Model function for CIFAR-10.\"\"\"\n",
    "  features = tf.reshape(features, [-1, _HEIGHT, _WIDTH, _NUM_CHANNELS])\n",
    "\n",
    "  learning_rate_fn = resnet_run_loop.learning_rate_with_decay(\n",
    "      batch_size=params['batch_size'], batch_denom=128,\n",
    "      num_images=_NUM_IMAGES['train'], boundary_epochs=[100, 150, 200],\n",
    "      decay_rates=[1, 0.1, 0.01, 0.001])\n",
    "\n",
    "  # We use a weight decay of 0.0002, which performs better\n",
    "  # than the 0.0001 that was originally suggested.\n",
    "  weight_decay = 2e-4\n",
    "\n",
    "  # Empirical testing showed that including batch_normalization variables\n",
    "  # in the calculation of regularized loss helped validation accuracy\n",
    "  # for the CIFAR-10 dataset, perhaps because the regularization prevents\n",
    "  # overfitting on the small data set. We therefore include all vars when\n",
    "  # regularizing and computing loss during training.\n",
    "  def loss_filter_fn(_):\n",
    "    return True\n",
    "\n",
    "  return resnet_run_loop.resnet_model_fn(\n",
    "      features=features,\n",
    "      labels=labels,\n",
    "      mode=mode,\n",
    "      model_class=Cifar10Model,\n",
    "      resnet_size=params['resnet_size'],\n",
    "      weight_decay=weight_decay,\n",
    "      learning_rate_fn=learning_rate_fn,\n",
    "      momentum=0.9,\n",
    "      data_format=params['data_format'],\n",
    "      resnet_version=params['resnet_version'],\n",
    "      loss_scale=params['loss_scale'],\n",
    "      loss_filter_fn=loss_filter_fn,\n",
    "      dtype=params['dtype']\n",
    "  )\n",
    "\n",
    "\n",
    "def define_cifar_flags():\n",
    "  resnet_run_loop.define_resnet_flags()\n",
    "  flags.adopt_module_key_flags(resnet_run_loop)\n",
    "  flags_core.set_defaults(data_dir='/tmp/cifar10_data',\n",
    "                          model_dir='/tmp/cifar10_model',\n",
    "                          resnet_size='32',\n",
    "                          train_epochs=250,\n",
    "                          epochs_between_evals=10,\n",
    "                          batch_size=128)\n",
    "\n",
    "\n",
    "def run_cifar(flags_obj):\n",
    "  \"\"\"Run ResNet CIFAR-10 training and eval loop.\n",
    "\n",
    "  Args:\n",
    "    flags_obj: An object containing parsed flag values.\n",
    "  \"\"\"\n",
    "  input_function = (flags_obj.use_synthetic_data and get_synth_input_fn()\n",
    "                    or input_fn)\n",
    "  resnet_run_loop.resnet_main(\n",
    "      flags_obj, cifar10_model_fn, input_function, DATASET_NAME,\n",
    "      shape=[_HEIGHT, _WIDTH, _NUM_CHANNELS])\n",
    "\n",
    "\n",
    "def main(_):\n",
    "  with logger.benchmark_context(flags.FLAGS):\n",
    "    run_cifar(flags.FLAGS)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  tf.logging.set_verbosity(tf.logging.INFO)\n",
    "  define_cifar_flags()\n",
    "  absl_app.run(main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
